# ST_ETL_py

ETL (Extract, Transform, Load) pipeline framework in Python.

## Project Overview
- **Name**: ST_ETL_py
- **Goal**: Modular and extensible ETL pipeline implementation
- **Status**: ðŸš§ In development

## Project Structure
```
ST_ETL_py/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/           # Data extraction modules
â”‚   â”‚   â”œâ”€â”€ base.py        # BaseExtractor abstract class
â”‚   â”‚   â”œâ”€â”€ csv_extractor.py
â”‚   â”‚   â””â”€â”€ api_extractor.py
â”‚   â”œâ”€â”€ transform/         # Data transformation modules
â”‚   â”‚   â””â”€â”€ base.py        # DataFrameTransformer with chaining
â”‚   â”œâ”€â”€ load/              # Data loading modules
â”‚   â”‚   â”œâ”€â”€ base.py        # BaseLoader abstract class
â”‚   â”‚   â”œâ”€â”€ csv_loader.py
â”‚   â”‚   â””â”€â”€ database_loader.py
â”‚   â”œâ”€â”€ utils/             # Utility modules
â”‚   â”‚   â”œâ”€â”€ logging_config.py
â”‚   â”‚   â””â”€â”€ config_loader.py
â”‚   â””â”€â”€ pipeline.py        # ETL orchestrator
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/              # Unit tests
â”‚   â””â”€â”€ integration/       # Integration tests
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.json        # Default configuration
â”‚   â””â”€â”€ .env.example       # Environment variables template
â”œâ”€â”€ data/                  # Data directory (gitignored)
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ staging/
â””â”€â”€ logs/                  # Log files (gitignored)
```

## Installation

```bash
# Clone repository
git clone https://github.com/Purple-Onion/ST_ETL_py.git
cd ST_ETL_py

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e ".[dev]"
```

## Usage

### Basic Pipeline Example

```python
from src.pipeline import ETLPipeline
from src.extract.csv_extractor import CSVExtractor
from src.transform.base import DataFrameTransformer
from src.load.csv_loader import CSVLoader

# Configure pipeline
pipeline = ETLPipeline(name="my_pipeline")

# Add extractor
pipeline.add_extractor(CSVExtractor("data/raw/input.csv"))

# Add transformer with chained operations
transformer = DataFrameTransformer()
transformer \
    .drop_duplicates() \
    .drop_na(subset=['name']) \
    .rename_columns({'id': 'user_id'})
pipeline.add_transformer(transformer)

# Add loader
pipeline.add_loader(CSVLoader("data/processed/output.csv"))

# Run pipeline
result = pipeline.run()
```

### Custom Extractor

```python
from src.extract.base import BaseExtractor

class MyExtractor(BaseExtractor):
    def connect(self):
        # Establish connection
        pass
    
    def extract(self, **kwargs):
        # Extract data
        return data
    
    def disconnect(self):
        # Clean up
        pass
```

## Running Tests

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src --cov-report=html

# Run specific test file
pytest tests/unit/test_extractors.py
```

## Components

### Extractors
- `CSVExtractor`: Extract data from CSV files
- `APIExtractor`: Extract data from REST APIs

### Transformers
- `DataFrameTransformer`: Chainable transformations for pandas DataFrames
  - `drop_duplicates()`, `drop_na()`, `rename_columns()`, `select_columns()`, `filter_rows()`

### Loaders
- `CSVLoader`: Load data to CSV files
- `DatabaseLoader`: Load data to SQL databases via SQLAlchemy

## Configuration

### Environment Variables
```bash
ETL_DATABASE_URL=postgresql://user:password@localhost:5432/db
ETL_API_KEY=your_api_key
ETL_LOG_LEVEL=INFO
```

### Config File (config/config.json)
```json
{
    "pipeline": {"name": "default_pipeline"},
    "extract": {"batch_size": 1000},
    "transform": {"drop_duplicates": true},
    "load": {"if_exists": "append"}
}
```

## License
MIT
